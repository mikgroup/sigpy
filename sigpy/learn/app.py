'''
Machine Learning Apps.
'''
import pickle
import logging
import sigpy as sp
        

class ConvSparseDecom(sp.app.LinearLeastSquares):
    r"""Convolutional sparse decomposition app.

    Considers the convolutional sparse linear model :math:`y_j = \sum_i l_i * r_{ij}`, 
    with :math:`l` fixed, and the problem,

    .. math:: 
        \min_{r_{ij}} \sum_j \frac{1}{2}\|y_j - \sum_i l_i * r_{ij}\|_2^2 + \lambda \|r_{ij}\|_1^2
    where :math:`y_j` is the jth data, :math:`l_i` is the ith filter, 
    :math:`r_{ij}` is the ith coefficient for jth data.

    Args:
        data (array): data array, the first dimension is the number of data.
            If multi_channel is True, then the second dimension should be the number of channels.
        l (array): filter array. If multi_channel is True,
            the first dimension is the number of filters. Otherwise, the first dimension
            is the number of channels, and second dimension is the number of filters.
        lamda (float): regularization parameter.
        mode (str): convolution mode in forward model. {'full', 'valid'}.
        multi_channel (bool): whether data is multi-channel or not.
        **kwargs: other LinearLeastSquares arguments.

    Returns:
        array: Coefficients. First two dimensions are number of data and number of filters.

    See Also:
        :func:`sigpy.app.LinearLeastSquares`

    """

    def __init__(self, data, l, lamda=0.001,
                 mode='full', multi_channel=False, device=sp.util.cpu_device, **kwargs):

        filt_width = l.shape[-1]
        if multi_channel:
            num_filters = l.shape[1]
            ndim = len(data.shape) - 2
        else:
            num_filters = l.shape[0]
            ndim = len(data.shape) - 1
        
        num_data = len(data)
        r_j_shape = _get_csc_r_j_shape(data.shape, num_data, num_filters,
                                       filt_width, mode, multi_channel)
        self.r_j = sp.util.zeros(r_j_shape, dtype=data.dtype, device=device)
        
        A_l = _get_csc_A_l(self.r_j, l, mode, multi_channel)
        proxg = sp.prox.L1L2Reg(A_l.ishape, lamda, axes=range(-ndim, 0))
        
        super().__init__(A_l, data, self.r_j, proxg=proxg, **kwargs)

    def _init(self):
        with sp.util.get_device(self.r_j):
            self.r_j.fill(0)
            
        super()._init()


class ConvSparseCoding(sp.app.App):
    r"""Convolutional sparse coding application.

    Considers the convolutional sparse bilinear model :math:`y_j = \sum_i l_i * r_{ij}`,
    and the objective function

    .. math:: 
        f(l, c) = 
        \sum_j \frac{1}{2} \|y_j - \sum_i l_i * r_{ij}\|_2^2 
        + \frac{\lambda}{2} \sum_i (\|l_i\|_2^2 + \|r_{ij}\|_1^2)
    where :math:`y_j` is the jth data, :math:`l_i` is the ith filter, 
    :math:`r_{ij}` is the ith coefficient for jth data.

    Args:
        data (array): data array, the first dimension is the number of data.
            If multi_channel is True, then the second dimension should be the number of channels.
        num_filters (int): number of filters.
        filt_width (int): filter widith.
        batch_size (int): batch size.
        lamda (float): regularization parameter.
        alpha (float): step-size.
        max_inner_iter (int): maximum number of iteration for inner-loop.
        max_power_iter (int): maximum number of iteration for power method.
        mode (str): convolution mode in forward model. {'full', 'valid'}.
        multi_channel (bool): whether data is multi-channel or not.
        **kwargs: other LinearLeastSquares arguments.

    Returns:
        array: Filters.

    See Also:
        :func:`sigpy.learn.app.ConvSparseDecom`

    References:
        Hashimoto, W., & Kurata, K. (2000).
        Properties of basis functions generated by shift invariant sparse
        representations of natural images. Biological Cybernetics, 83(2), 111â€“118.

    """

    def __init__(self, data, num_filters, filt_width, batch_size,
                 lamda=0.001, alpha=1, init_scale=1e-5,
                 max_inner_iter=100, max_power_iter=10, max_iter=100,
                 mode='full', multi_channel=False, device=sp.util.cpu_device):
    
        dtype = data.dtype
        self.data = data
        
        self.batch_size = batch_size
        num_batches = len(data) // batch_size
        self.j_idx = sp.index.ShuffledIndex(num_batches)
        self.data_j = sp.util.empty((batch_size, ) + data.shape[1:], dtype=dtype, device=device)
        
        l_shape = _get_csc_l_shape(data.shape, num_filters, filt_width, mode, multi_channel)
        self.l = sp.util.empty(l_shape, dtype=dtype, device=device)
        self.init_scale = init_scale

        min_r_j_app = ConvSparseDecom(self.data_j, self.l, lamda=lamda,
                                      mode=mode, multi_channel=multi_channel,
                                      max_power_iter=max_power_iter,
                                      max_iter=max_inner_iter, device=device)
        self.r_j = min_r_j_app.r_j
        
        A_r_j = _get_csc_A_r_j(self.r_j, self.l, mode, multi_channel)
        min_l_app = sp.app.LinearLeastSquares(A_r_j, self.data_j, self.l,
                                              mu=1 / (alpha * num_batches), z=self.l,
                                              lamda=lamda / num_batches,
                                              max_iter=max_inner_iter)

        alg = sp.alg.AltMin(min_r_j_app.run, min_l_app.run, max_iter=max_iter)

        super().__init__(alg)

    def _init(self):
        sp.util.move_to(self.l, sp.util.randn_like(self.l, scale=self.init_scale))

    def _pre_update(self):
        j = self.j_idx.next()
        j_start = j * self.batch_size
        j_end = (j + 1) * self.batch_size
        
        sp.util.move_to(self.data_j, self.data[j_start:j_end])

    def _output(self):
        return self.l

    
class LinearRegression(sp.app.LinearLeastSquares):
    r"""Performs linear regression to fit input to output.

    Considers the linear model :math:`y_j = M x_j`, and the problem,

    .. math::
        \min_M \sum_j \frac{1}{2} \| y_j - M x_j \|_2^2
    where :math:`y_j` is the jth output, :math:`M` is the learned matrix,
    and :math:`x_j` is the jth input.

    Args:
        input (array): input data of shape (num_data, ...).
        output (array): output data of shape (num_data, ...).
        batch_size (int): batch size.
        alpha (float): step size.

    Returns:
       array: matrix of shape input.shape[1:] + output.shape[1:].

    """

    def __init__(self, input, output, batch_size, alpha,
                 max_iter=100, device=sp.util.cpu_device, **kwargs):
        
        dtype = output.dtype

        num_data = len(output)
        num_batches = num_data // batch_size
        self.batch_size = batch_size
        self.input = input
        self.output = output

        mat = sp.util.zeros(input.shape[1:] + output.shape[1:], dtype=dtype, device=device)
        
        self.j_idx = sp.index.ShuffledIndex(num_batches)
        self.input_j = sp.util.empty((batch_size, ) + input.shape[1:],
                                     dtype=dtype, device=device)
        self.output_j = sp.util.empty((batch_size, ) + output.shape[1:],
                                      dtype=dtype, device=device)
        
        A = _get_lr_A(self.input_j, self.output_j, mat, batch_size)
        
        super().__init__(A, self.output_j, mat, alg_name='GradientMethod', accelerate=False,
                         alpha=alpha, max_iter=max_iter)
        
    def _pre_update(self):
        j = self.j_idx.next()
        j_start = j * self.batch_size
        j_end = (j + 1) * self.batch_size
        
        sp.util.move_to(self.input_j, self.input[j_start:j_end])
        sp.util.move_to(self.output_j, self.output[j_start:j_end])

        
def _get_lr_A(input_j, output_j, mat, batch_size):
    input_j_size = sp.util.prod(input_j.shape[1:])
    output_j_size = sp.util.prod(output_j.shape[1:])
    
    Ri = sp.linop.Reshape([input_j_size, output_j_size], mat.shape)
    M = sp.linop.MatMul([input_j_size, output_j_size], input_j.reshape([batch_size, -1]))
    Ro = sp.linop.Reshape(output_j.shape, [batch_size, output_j_size])

    A = Ro * M * Ri

    return A


class ConvSparseCoefficients(object):
    r"""Convolutional sparse coefficients.

    Generates coefficients on the fly using convolutional sparse decomposition.
    ConvSparseCoefficients can be sliced like arrays.

    Args:
        data (array): data array, the first dimension is the number of data.
            If multi_channel is True, then the second dimension should be the number of channels.
        l (array): filter. If multi_channel is True,
            the first dimension is the number of filters. Otherwise, the first dimension
            is the number of channels, and second dimension is the number of filters.
        lamda (float): regularization parameter.
        mode (str): convolution mode in forward model. {'full', 'valid'}.
        multi_channel (bool): whether data is multi-channel or not.
        max_iter (bool): maximum number of iterations.
        max_power_iter (bool): maximum number of power iterations.

    Attributes:
        shape (tuple of ints): coefficient shape.
        ndim (int): number of dimensions of coefficient.
        dtype (Dtype): Data type.

    """

    def __init__(self, data, l,
                 lamda=0.001, multi_channel=False, mode='full',
                 max_iter=100, max_power_iter=10, device=sp.util.cpu_device):
        
        self.data = data
        self.l = l
        self.lamda = lamda
        self.multi_channel = multi_channel
        self.mode = mode
        self.max_iter = max_iter
        self.max_power_iter = max_power_iter

        if multi_channel:
            num_filters = l.shape[1]
        else:
            num_filters = l.shape[0]
            
        filt_width = l.shape[-1]
        self.shape = _get_csc_r_j_shape(
            data.shape, len(data), num_filters, filt_width, mode, multi_channel)
        self.ndim = len(self.shape)
        self.device = sp.util.Device(device)
        self.dtype = data.dtype
        
    def __getitem__(self, slc):
        if isinstance(slc, int):
            data_j = self.data[slc:(slc + 1)]
            slc = 0

        elif isinstance(slc, slice):
            data_j = self.data[slc]
            slc = slice(None)

        elif isinstance(slc, tuple) or isinstance(slc, list):
            if isinstance(slc[0], int):
                data_j = self.data[slc[0]:(slc[0] + 1)]
                slc = [0] + list(slc[1:])
            else:
                data_j = self.data[slc[0]]
                slc = [slice(None)] + list(slc[1:])

        data_j = sp.util.move(data_j, self.device)
        l = sp.util.move(self.l, self.device)
        app_j = ConvSparseDecom(data_j, l, lamda=self.lamda,
                                multi_channel=self.multi_channel, mode=self.mode,
                                max_iter=self.max_iter, max_power_iter=self.max_power_iter,
                                device=self.device)

        fea = app_j.run()

        with self.device:
            return fea[slc]

    @classmethod
    def load(cls, filename):
        with open(filename, "rb") as f:
            return pickle.load(f)

    def save(self, filename):
        self.use_device(sp.util.cpu_device)
        with open(filename, "wb") as f:
            pickle.dump(self, f)


def _get_csc_update_l(A_r_j, l, data_j, num_batches, alpha, lamda):

    device = sp.util.get_device(l)
    def update_l():
        with device:
            gradf_l = A_r_j.H(A_r_j(l) - data_j)
            gradf_l *= num_batches
            sp.util.axpy(gradf_l, lamda, l)
            
            sp.util.axpy(l, -alpha, gradf_l)

    return update_l


def _get_csc_r_j_shape(data_shape, num_data, num_filters, filt_width, mode, multi_channel):
    if multi_channel:
        ndim = len(data_shape) - 2
    else:
        ndim = len(data_shape) - 1

    if mode == 'full':
        r_j_shape = tuple([num_data, num_filters] +
                          [i - min(i, filt_width) + 1 for i in data_shape[-ndim:]])
    else:
        r_j_shape = tuple([num_data, num_filters] +
                          [i + min(i, filt_width) - 1 for i in data_shape[-ndim:]])

    return r_j_shape


def _get_csc_A_l(r_j, l, mode, multi_channel):
    
    if sp.util.get_device(l) != sp.util.cpu_device and sp.config.cudnn_enabled:
        if multi_channel:
            l_cudnn_shape = l.shape
        else:
            l_cudnn_shape = (1, ) + l.shape
            
        A_l = sp.linop.CudnnConvolveData(r_j.shape, l.reshape(l_cudnn_shape), mode=mode)
        
        if not multi_channel:
            data_shape = [A_l.oshape[0]] + A_l.oshape[2:]
            R_y = sp.linop.Reshape(data_shape, A_l.oshape)
        
            A_l = R_y * A_l
    else:
        if multi_channel:
            ndim = l.ndim - 2
        else:
            ndim = l.ndim - 1
            
        C_l = sp.linop.Convolve(r_j.shape, l, axes=range(-ndim, 0), mode=mode)
        S_r_j = sp.linop.Sum(C_l.oshape, axes=[-(ndim + 1)])

        A_l = S_r_j * C_l

    return A_l


def _get_csc_l_shape(data_shape, num_filters, filt_width, mode, multi_channel):
    if multi_channel:
        ndim = len(data_shape) - 2
    else:
        ndim = len(data_shape) - 1

    if multi_channel:
        num_channels = data_shape[1]
        l_shape = tuple([num_channels, num_filters] +
                        [min(d, filt_width) for d in data_shape[-ndim:]])
    else:
        l_shape = tuple([num_filters] + [min(d, filt_width) for d in data_shape[-ndim:]])

    return l_shape


def _get_csc_A_r_j(r_j, l, mode, multi_channel):

    if sp.util.get_device(l) != sp.util.cpu_device and sp.config.cudnn_enabled:
        if multi_channel:
            l_cudnn_shape = l.shape
        else:
            l_cudnn_shape = (1, ) + l.shape
            
        R_l = sp.linop.Reshape(l_cudnn_shape, l.shape)
        C_r_j = sp.linop.CudnnConvolveFilter(l_cudnn_shape, r_j, mode=mode)
        
        A_r_j = C_r_j * R_l

        if not multi_channel:
            data_shape = [A_r_j.oshape[0]] + A_r_j.oshape[2:]
            R_y = sp.linop.Reshape(data_shape, A_r_j.oshape)

            A_r_j = R_y * A_r_j
    else:
        ndim = l.ndim - 1
        C_r_j = sp.linop.Convolve(l.shape, r_j, axes=range(-ndim, 0), mode=mode)
        S_l = sp.linop.Sum(C_r_j.oshape, axes=[1])

        A_r_j = S_l * C_r_j

    return A_r_j
